{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"data":{"text/plain":["<matplotlib.pyplot._IonContext at 0x28d72991c70>"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","\n","from __future__ import print_function, division\n","import os\n","import torch\n","import pandas as pd\n","from skimage import io, transform\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import torchvision.models as models\n","import torch.nn as nn\n","import time\n","from math import cos, pi\n","\n","plt.ion()   # interactive mode\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def show_landmarks(image, landmarks):\n","    \"\"\"Show image with landmarks\"\"\"\n","    plt.imshow(image, cmap='gray')\n","    plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n","    plt.pause(0.001)  # pause a bit so that plots are updated"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["class FaceLandmarksDataset(Dataset):\n","    \"\"\"Face Landmarks dataset.\"\"\"\n","\n","    def __init__(self, json_file, root_dir, transform=None):\n","        \"\"\"\n","        Args:\n","            json_file (string): Path to the json file with annotations.\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.landmarks_frame = pd.read_json(json_file)\n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.landmarks_frame)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","        img_name = os.path.join(self.root_dir, self.landmarks_frame.iloc[idx]['image'])\n","        image = io.imread(img_name)\n","        landmarks = self.landmarks_frame.iloc[idx]['landmarks']\n","        landmarks = np.array([landmarks])\n","        landmarks = landmarks.astype('float').reshape(-1, 2)\n","        sample = {'image': image, 'landmarks': landmarks}\n","\n","        if self.transform:\n","            sample = self.transform(sample)\n","\n","        return sample"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class Rescale(object):\n","    \"\"\"Rescale the image in a sample to a given size.\n","\n","    Args:\n","        output_size (tuple or int): Desired output size. If tuple, output is\n","            matched to output_size. If int, smaller of image edges is matched\n","            to output_size keeping aspect ratio the same.\n","    \"\"\"\n","\n","    def __init__(self, output_size):\n","        assert isinstance(output_size, (int, tuple))\n","        self.output_size = output_size\n","\n","    def __call__(self, sample):\n","        image, landmarks = sample['image'], sample['landmarks']\n","\n","        h, w = image.shape[:2]\n","        if isinstance(self.output_size, int):\n","            if h > w:\n","                new_h, new_w = self.output_size * h / w, self.output_size\n","            else:\n","                new_h, new_w = self.output_size, self.output_size * w / h\n","        else:\n","            new_h, new_w = self.output_size\n","\n","        new_h, new_w = int(new_h), int(new_w)\n","\n","        img = transform.resize(image, (new_h, new_w))\n","\n","        # h and w are swapped for landmarks because for images,\n","        # x and y axes are axis 1 and 0 respectively\n","        landmarks = landmarks * [new_w / w, new_h / h]\n","\n","        return {'image': img, 'landmarks': landmarks}"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class ToTensor(object):\n","    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n","\n","    def __call__(self, sample):\n","        image, landmarks = sample['image'], sample['landmarks']\n","\n","        # swap color axis because\n","        # numpy image: H x W x C\n","        # torch image: C x H x W\n","        # print(image.shape) # (240, 240)\n","        tensor = torch.from_numpy(image)\n","        tensor = torch.stack([tensor, tensor, tensor], dim=0)\n","        return {'image': tensor,\n","                'landmarks': torch.from_numpy(landmarks)}"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["face_dataset = FaceLandmarksDataset(\n","    json_file='preprocessed_dataset/dataset.json', \n","    root_dir='preprocessed_dataset', \n","    transform=transforms.Compose([Rescale(224), ToTensor()])\n",")\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def get_train_dataloader():\n","    if torch.distributed.is_initialized():\n","        train_sampler = torch.utils.data.distributed.DistributedSampler(face_dataset)\n","    else:\n","        train_sampler = None\n","    return DataLoader(face_dataset, batch_size=100, shuffle=True, num_workers=0, pin_memory=True, sampler=train_sampler)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["\n","train_loader = get_train_dataloader()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def show_landmarks_batch(sample_batched):\n","    \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n","    images_batch, landmarks_batch = \\\n","            sample_batched['image'], sample_batched['landmarks']\n","    batch_size = len(images_batch)\n","    im_size = images_batch.size(2)\n","    grid_border_size = 1\n","\n","    grid = utils.make_grid(images_batch)\n","    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n","\n","    for i in range(batch_size):\n","        plt.scatter(landmarks_batch[i, :, 0].numpy() + i * im_size + (i + 1) * grid_border_size,\n","                    landmarks_batch[i, :, 1].numpy() + grid_border_size,\n","                    s=10, marker='.', c='r')\n","\n","        plt.title('Batch from dataloader')\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["for i_batch, batch in enumerate(train_loader):\n","    print(i_batch, batch['image'].size(),\n","          batch['landmarks'].size())\n","    if i_batch == 3:\n","        plt.figure()\n","        show_landmarks_batch(batch)\n","        plt.axis('off')\n","        plt.ioff()\n","        plt.show()\n","        break"]}],"metadata":{"interpreter":{"hash":"bdb84a89bbce116652f5506d99dd8f1187909df9315f03ed4e59ccea216d4f8b"},"kernelspec":{"display_name":"Python 3.8.11 64-bit ('.venv-nix')","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":4}
